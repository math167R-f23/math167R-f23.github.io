{
  "hash": "301621e7787f98b2c7b26a4e48717367",
  "result": {
    "markdown": "---\ntitle: \"Check-in 9\"\nauthor: \"YOUR NAME HERE\"\ndate: \"2023-11-06\"\ndate-format: \"[Due] MMMM DD, YYYY\"\nformat: \n  html:\n    embed-resources: true\n    code-tools: true\n    code-summary: \"Code\"\n---\n\n\nRemember, you must submit *both* your .Rmd and the compiled .html in order to receive full credit! In addition, to receive full credit, your code output and plots must be correctly formatted.\n\n### Collaborators\n\nINSERT NAMES OF ANY COLLABORATORS\n\n## Part 1. Training and Test Error (10 points)\n\nUse the following code to generate data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n# generate data\nset.seed(302)\nn <- 30\nx <- sort(runif(n, -3, 3))\ny <- 2*x + 2*rnorm(n)\nx_test <- sort(runif(n, -3, 3))\ny_test <- 2*x_test + 2*rnorm(n)\ndf_train <- data.frame(\"x\" = x, \"y\" = y)\ndf_test <- data.frame(\"x\" = x_test, \"y\" = y_test)\n\n# store a theme\nmy_theme <- theme_bw(base_size = 16) + \n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"),\n        plot.subtitle = element_text(hjust = 0.5))\n\n# generate plots\ng_train <- ggplot(df_train, aes(x = x, y = y)) + geom_point() +\n  xlim(-3, 3) + ylim(min(y, y_test), max(y, y_test)) + \n  labs(title = \"Training Data\") + my_theme\ng_test <- ggplot(df_test, aes(x = x, y = y)) + geom_point() +\n  xlim(-3, 3) + ylim(min(y, y_test), max(y, y_test)) + \n  labs(title = \"Test Data\") + my_theme\ng_train\n```\n\n::: {.cell-output-display}\n![](10_checkin_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\ng_test\n```\n\n::: {.cell-output-display}\n![](10_checkin_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n:::\n\n\n**1a.** For every k in between 1 and 10, fit a degree-k polynomial linear regression model with `y` as the response and `x` as the explanatory variable(s).\n(*Hint: Use *`poly()`*, as in the lecture slides.*)\n\n**1b.** For each model from (a), record the training error. Then predict `y_test` using `x_test` and also record the test error.\n\n**1c.** Present the 10 values for both training error and test error on a single table. Comment on what you notice about the relative magnitudes of training and test error, as well as the trends in both types of error as $k$ increases.\n\n**1d.** If you were going to choose a model based on training error, which would you choose? Plot the data, colored by split. Add a line to the plot representing your selection for model fit. Add a subtitle to this plot with the (rounded!) test error.\n(*Hint: See Lecture Slides 9 for example code.*)\n\n**1e.** If you were going to choose a model based on test error, which would you choose? Plot the data, colored by split. Add a line to the plot representing your selection for model fit. Add a subtitle to this plot with the (rounded!) test error.\n\n**1f.** What do you notice about the shape of the curves from part (d) and (e)? Which model do you think has lower bias? Lower variance? Why?\n\n## Part 2. k-Nearest Neighbors Cross-Validation (10 points)\n\nFor this part, note that there are tidyverse methods to perform cross-validation in R (see the `rsample` package). However, your goal is to understand and be able to implement the algorithm \"by hand\", meaning  that automated procedures from the `rsample` package, or similar packages, will not be accepted.\n\nTo begin, load in the popular `penguins` data set from the package `palmerpenguins`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\ndata(package = \"palmerpenguins\")\n```\n:::\n\n\nOur goal here is to predict output class `species` using covariates `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`.\nAll your code should be within a function `my_knn_cv`.\n\n**Input:**\n\n  * `train`: input data frame\n  * `cl`: true class value of your training data\n  * `k_nn`: integer representing the number of neighbors\n  * `k_cv`: integer representing the number of folds\n  \n*Please note the distinction between `k_nn` and `k_cv`!*\n\n**Output:** a list with objects\n\n  * `class`: a vector of the predicted class $\\hat{Y}_{i}$ for all observations\n  * `cv_err`: a numeric with the cross-validation misclassification error\n\n\nYou will need to include the following steps:\n\n* Within your function, define a variable `fold` that randomly assigns observations to folds $1,\\ldots,k$ with equal probability. (*Hint: see the example code on the slides for k-fold cross validation*)\n* Iterate through $i = 1:k$. \n  * Within each iteration, use `knn()` from the `class` package to predict the class of the $i$th fold using all other folds as the training data.\n  * Also within each iteration, record the prediction and the misclassification rate (a value between 0 and 1 representing the proportion of observations that were classified **incorrectly**).\n* After you have done the above steps for all $k$ iterations, store the vector `class` as the output of `knn()` with the full data as both the training and the test data, and the value `cv_error` as the average misclassification rate from your cross validation.\n\n**Submission:** To prove your function works, apply it to the `penguins` data. Predict output class `species` using covariates `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`. Use $5$-fold cross validation (`k_cv = 5`). Use a table to show the `cv_err` values for 1-nearest neighbor and 5-nearest neighbors (`k_nn = 1` and `k_nn = 5`). Comment on which value had lower CV misclassification error and which had lower training set error (compare your output `class` to the true class, `penguins$species`).\n",
    "supporting": [
      "10_checkin_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}