{
  "hash": "fa4c318399eb73e442362473e44a6e5c",
  "result": {
    "markdown": "---\ntitle: \"MATH167R: Linear regression\"\nauthor: \"Peter Gao\"\nformat: \n  revealjs:\n    theme: [./slides.scss, ../theme.scss]\neditor: visual\n---\n\n\n## Overview of today\n\n-   Covariance and correlation\n-   Linear regression\n\n## Relating numerical variables\n\n\n\n\n## Regression methods\n\nLet's assume we have some data $X_1,\\ldots,X_p, Y$ where $X_1,\\ldots,X_p$ are $p$ **independent variables/explanatory variables/covariates/predictors** and $Y$ is the **dependent variables/response/outcome**.\nWe want to know the relationship between our covariates and our response, we can do this with a method called **regression**.\nRegression provides us with a statistical method to conduct inference and prediction.\n\n## Regression methods\n\n* **inference:** assess the relationship between our variables, our statistical model as a whole, predictor importance\n  * What is the relationship between sleep and GPA?\n  * Is parents' education or parents' income more important for explaining income?\n* **prediction:** predict new/future outcomes from new/future covariates\n  * Can we predict test scores based on hours spent studying?\n\n## Why is it called regression?\n\n\n## Linear Regression\n\nGiven our response $Y$ and our predictors $X_1, \\ldots, X_p$, a **linear regression model** takes the form:\n\n$$\n\\begin{align}\nY &= \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p + \\epsilon,\\\\\n\\epsilon &\\sim N(0,\\sigma^2) \n\\end{align}\n$$\n\n## Matrix Notation\n\nWe can fully write out a linear regression model\n\n$$\n\\begin{equation}\n\\begin{bmatrix} y_1 \\\\ y_2\\\\ \\vdots \\\\ y_n \\end{bmatrix} = \n\\begin{bmatrix} 1 & x_{1,1} & \\cdots & x_{1,k}\\\\\n1 & x_{2,1} & \\cdots & x_{2, k}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n,1} & \\cdots & x_{n, k}\\end{bmatrix}\n\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{k} \\end{bmatrix} +\n\\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_{n} \\end{bmatrix}\n\\end{equation}\n$$\n\nThis can also be expressed in matrix form:\n\n$$\n\\begin{align}\n\\mathbf{Y} &= \\mathbf{X}\\beta + \\epsilon,\\\\\n\\epsilon&\\sim N(0,1)\n\\end{align}\n$$\n\n* $\\mathbf{Y} \\in \\mathbb{R}^{n \\times 1}$: an n-dimensional vector of the response\n* $\\mathbf{X} \\in \\mathbb{R}^{n \\times (k+1)}$: a $((k+1)\\times n)$ matrix of the predictors (including intercept)\n* $\\beta \\in \\mathbb{R}^{((k+1)\\times 1)}$: a $(k+1)$-dimensional vector of regression parameters\n* $\\epsilon \\in \\mathbb{R}^{n \\times 1}$: an n-dimensional vector of the error term\n\n## $\\large \\epsilon$: Error term\n\n$\\epsilon$, pronounced epsilon, represents the **error term** of our model.\nWe can model $Y$ as a linear function of the $X$'s, but in the real world, the relationship won't always be perfect. There is noise! It can come from\n\n* Measurement error in the $X$'s\n* Measurement error in the $Y$'s\n* Unobserved/missing variables in the model\n* Deviations in the true model from linearity\n* True randomness\n\nIn linear regression, we assume that this error term is normally distributed with mean zero and variance $\\sigma^2$.\n\n## $\\large \\beta_0$: Intercept\n\n$\\beta_0$ is the **intercept term** of our model. Notice that \n\n$$\\large \\mathbb{E}[Y|X_1 = X_2 = \\cdots = X_p = 0] = \\beta_0$$\n\nThus, $\\beta_0$ is the expected value of our response if all the covariates are equal to $0$!\nThis is also known as the y-intercept of our model.\n\n---\n\n# $\\large X_j$: Independent variable\n\n$X_j$ represents the $j$<sup>th</sup> independent variable in our model. Notice that \n$$\\large \\mathbb{E}[Y|X_1,\\ldots, X_p] = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p$$\nWhat happens to this expectation if we increase $X_j$ by 1 unit, holding everything else constant?\n\n--\n\nThe conditional expectation of $Y$ increases by $\\beta_j$!\n\n---\n\n# $\\large \\beta_j$: Coefficient\n\n$\\beta_j$ represents the $j$<sup>th</sup> regression coefficient in our model.\nFrom the previous slide, we saw that for every 1 unit increase in $X_j$, holding all other variables constant, the expected value of the response increases by $\\beta_j$.\nFrom this we can derive an interpretation.\n\n**Interpretation of $\\beta_j$:** the expected difference in the response between two observations differing by one unit in $X_j$, with all other covariates identical.\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}